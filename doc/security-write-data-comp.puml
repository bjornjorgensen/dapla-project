@startuml

[Zeppelin\nSpark\nInterpreter] as interpreter
[Zeppelin] as zeppelin
[dataset-access] as da_s
[catalog] as catalog
[lineage] as lineage
[spark-service] as sp_s
[broker] as broker
database "Cloud Storage" as gcs {
}
frame "Executor" as worker {
  frame "Task" as task {
    [datasource] as datasource
    [gcs connector] as gcs_connector
  }
}

zeppelin -u-> interpreter : 1. submit job\n (user-token)
interpreter -> worker : 2. distribute job\n (user-token)
datasource --> sp_s : 3. prepare write\n (user-token)
sp_s --> catalog : 4. get metadata\n (user-token)
gcs_connector --> broker : 5. get access-token\n (user-token)
broker --> catalog : 6. get metadata\n (user-token)
broker --> da_s : 7. has-access?\n (user-token)
gcs_connector ---> gcs : 8. write dataset\n (access-token)
datasource -> sp_s : 9. write metadata\n (user-token)
sp_s -> catalog : 10. write metadata\n (user-token)
catalog -> da_s : 11. has access?\n (user-token)
sp_s --> lineage : 12. write lineage?\n (user-token)
lineage --> da_s : 13. has access?\n (user-token)

@enduml