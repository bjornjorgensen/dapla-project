@startuml

  package "dataset-access" {
    () "GRPC" as da_proto
    [dataset-access-service] as da_s
    da_s - da_proto
    da_s --> da_pg
    database "Postgres" as da_pg {
    }
  }

  package "catalog" {
    () "GRPC" as ca_proto
    [catalog-service] as ca_s
    ca_s - ca_proto
    ca_s -> da_proto
    ca_s --> ca_bt
    database "Bigtable" as ca_bt {
    }
  }

  package "pseudo-keys" {
    () "GRPC" as pk_proto
    [secret-service] as ss_s
    pk_proto - ss_s
    ss_s --> ss_pg
    ss_s --> da_proto
    database "Postgres" as ss_pg {
    }
  }

  package "lineage" {
    () "GRPC" as lin_proto
    [lineage] as lin_s
    lin_s - lin_proto
    lin_s --> lin_neo
    lin_s --> da_proto
    database "Neo4j" as lin_neo {
    }
  }

  package "spark-service" {
    () "HTTP" as ss_http
    [spark-service] as sp_s
    sp_s - ss_http
    sp_s --> ca_proto
    sp_s ---> da_proto
    sp_s --> pk_proto
    sp_s --> lin_proto
  }

  package "data-access" {
    () "HTTP" as broker_http
    [broker] as br_s
    br_s - broker_http
    br_s --> da_proto
  }

  database "Google\nCloud Storage" as gcs {
  }

  cloud "dataproc" as pkg_haadop {
    [Yarn\nResource-\nManager] as yarn
    node "Master" as masternode {
      frame "Zeppelin\nSpark\nInterpreter" as interpreter {
        frame "Job" as job {
        }
      }
    }
    node "Worker" as worker {
      frame "Spark Executor" as executor {
        frame "Spark Task" as task {
          [datasource] as datasource
          datasource --> ss_http
          [gcs connector] as gcs_connector_w
          gcs_connector_w ---> gcs
          gcs_connector_w ----> broker_http
          datasource --> broker_http
        }
      }
    }
  }

  package "zeppelin" {
    [Zeppelin] -- interpreter
  }


@enduml