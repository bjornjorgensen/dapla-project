{
  "paragraphs": [
    {
      "title": "Create dataset",
      "text": "%spark\n\ncase class SomeRec(fnr: String, navn: String, postnr: String, antall: Int)\n\nval raw_data \u003d sc.parallelize(Seq(\n        SomeRec(\"23028700172\", \"Donald Duck\", \"3158\", 42),\n        SomeRec(\"18098800057\", \"Dolly Duck\", \"3158\", 13),\n        SomeRec(\"09039500121\", \"Mikke Mus\", \"0123\", 37)\n    )).toDF()\n    \nraw_data.show()\n",
      "user": "user1",
      "dateUpdated": "2020-01-15 13:07:01.502",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+-----------+-----------+------+------+\n|        fnr|       navn|postnr|antall|\n+-----------+-----------+------+------+\n|23028700172|Donald Duck|  3158|    42|\n|18098800057| Dolly Duck|  3158|    13|\n|09039500121|  Mikke Mus|  0123|    37|\n+-----------+-----------+------+------+\n\nimport sqlContext.implicits._\ndefined class SomeRec\nraw_data: org.apache.spark.sql.DataFrame \u003d [fnr: string, navn: string ... 2 more fields]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1579006588219_1968773283",
      "id": "20200114-125628_902571226",
      "dateCreated": "2020-01-14 12:56:28.219",
      "dateStarted": "2020-01-15 13:07:01.591",
      "dateFinished": "2020-01-15 13:07:28.460",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Pseudonymize",
      "text": "%spark\nvar ns \u003d \"user1/pseudonymized-dataset\"\nvar pseudo_data \u003d raw_data.write\n    .format(\"gsim\")\n    .option(\"pseudo\", \"fnr\u003dfpe-fnr,navn\u003dfpe-text,postnr\u003dfpe-digits\")\n    .option(\"valuation\", \"INTERNAL\")\n    .option(\"state\", \"INPUT\")\n    .mode(\"overwrite\")\n    .save(ns)\n",
      "user": "user1",
      "dateUpdated": "2020-01-15 13:09:00.418",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Skriver datasett til: user1/pseudonymized-dataset\nOppretter datasett: file:///data/datastore/d79fdaf6-ef49-4014-b397-4de1e2907b8e.parquet\nns: String \u003d user1/pseudonymized-dataset\npseudo_data: Unit \u003d ()\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1579007287378_1127429804",
      "id": "20200114-130807_128334940",
      "dateCreated": "2020-01-14 13:08:07.378",
      "dateStarted": "2020-01-15 13:09:00.454",
      "dateFinished": "2020-01-15 13:09:01.087",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Read pseudonymized data",
      "text": "%spark\nvar ns \u003d \"user1/pseudonymized-dataset\"\nvar data \u003d spark.read\n    .format(\"gsim\")\n    .load(ns)\ndata.show()",
      "user": "user1",
      "dateUpdated": "2020-01-15 13:09:03.444",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Leser datasett fra: user1/pseudonymized-dataset\nBrukernavn: user1\nFant følgende datasett: file:///data/datastore/d79fdaf6-ef49-4014-b397-4de1e2907b8e.parquet\n+-----------+-----------+------+------+\n|        fnr|       navn|postnr|antall|\n+-----------+-----------+------+------+\n|19577523264|#Æe4x.:0OÃJ|  6220|    42|\n|55733985586| ;x#Öö\u003ePX?v|  6220|    13|\n|67170806531|  BaÆÃ\u003dazUD|  0284|    37|\n+-----------+-----------+------+------+\n\nns: String \u003d user1/pseudonymized-dataset\ndata: org.apache.spark.sql.DataFrame \u003d [fnr: string, navn: string ... 2 more fields]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1579008854847_334305658",
      "id": "20200114-133414_2047283581",
      "dateCreated": "2020-01-14 13:34:14.847",
      "dateStarted": "2020-01-15 13:09:03.483",
      "dateFinished": "2020-01-15 13:09:04.214",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Restore pseudonymized data",
      "text": "%spark\nvar ns \u003d \"user1/pseudonymized-dataset\"\nvar restored_data \u003d spark.read\n    .format(\"gsim\")\n    .option(\"pseudo\", \"fnr\u003dfpe-fnr,navn\u003dfpe-text,postnr\u003dfpe-digits\") // this will be fetched from catalog service\n    .load(ns)\nrestored_data.show()",
      "user": "user1",
      "dateUpdated": "2020-01-15 13:11:00.701",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Leser datasett fra: user1/pseudonymized-dataset\nBrukernavn: user1\nFant følgende datasett: file:///data/datastore/d79fdaf6-ef49-4014-b397-4de1e2907b8e.parquet\n+-----------+-----------+------+------+\n|        fnr|       navn|postnr|antall|\n+-----------+-----------+------+------+\n|19577523264|#Æe4x.:0OÃJ|  6220|    42|\n|55733985586| ;x#Öö\u003ePX?v|  6220|    13|\n|67170806531|  BaÆÃ\u003dazUD|  0284|    37|\n+-----------+-----------+------+------+\n\nns: String \u003d user1/pseudonymized-dataset\nrestored_data: org.apache.spark.sql.DataFrame \u003d [fnr: string, navn: string ... 2 more fields]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1579009627022_-1634868698",
      "id": "20200114-134707_1717651123",
      "dateCreated": "2020-01-14 13:47:07.022",
      "dateStarted": "2020-01-15 13:09:51.994",
      "dateFinished": "2020-01-15 13:09:52.700",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\n",
      "user": "anonymous",
      "dateUpdated": "2020-01-14 13:51:41.238",
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1579009901237_2076850416",
      "id": "20200114-135141_77159228",
      "dateCreated": "2020-01-14 13:51:41.237",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "samples/pseudo",
  "id": "sample-pseudo",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {
    "spark:user1:": []
  },
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}